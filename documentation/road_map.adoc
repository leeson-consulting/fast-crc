:source-highlighter: rouge
:rouge-style: monokai

= Fast-CRC: Road Map

== 1.x

The principle motivation of Fast-CRC was to provide a full catalogue of ready to use and C99 compliant CRC algorithms.
These provide consistent, correct, and efficient CRC implementations for engineers who just need to "get on with it".

This objective was met in the 1.0 release (circa April 2024),
which also included some novel algorithms and survey data for those wishing to explore further.

== 2.x

The second major release will focus on supporting multiple languages.
This will be done in phases.

The first phase (2.0) will target Rust, Golang, and Javascript based on their features and popularity as at the development date.
Rust is the only significant competitor to C and C++ in the Embedded / System Programing space.
Golang is the best (IMO) general purpose programming language and is very suitable to embedded Linux Server applications.
Javascript is deployed on scale for Web based Client Server applications.

The basic hybrid-generator approach of the library will remain, but with an additional code generator step.

In the 1.x implementation, scripts in the tools folder generate tightly coupled C source that are assembled at build time via the C preprocessor.

In the 2.x implementation, these scripts will be refactored to generate tightly coupled source for each language of interest.
The C preprocessor will be used to assemble these source to explicitly generate the final source for every language target except C,
which will continue to use the 1.x behaviour.

This approach aims to maximise code reuse and should apply equally well to any imperative programming language.

Any minor releases of the 2.x development line will aim to support more languages, eg. D, Java, C#, Python, etc.

== 3.x

The third major release will focus on faster kernels.

It must be said from the start that faster algorithms, don't necessarily offer practical benefits.
The 100s of microseconds saved calculating a CRC across a small data set doesn't always amount to much,
especially if the demand is infrequent like in low-bandwidth telemetry comms.

Nevertheless, in the quest for speed there are a number of avenues to investigate.
These are outlined below.

The key challenges in meeting the general objective of faster CRC implementations will be:

* Identifying and validating individual methods.
* Determining whether the methods should be opt-in or opt-out.
* Detecting which methods are supported on any given platform.
* Providing predictable fall-back strategies that hide the implementation details from the user.

=== Explicit Loop Unrolling

Explicit loop unrolling is top of the list for potential speed improvements.
Optimising compilers are very good at automating unrolling on byte at a time processing.
But there is room for manipulation via compiler flags.
Deliberate data pre-fetch instructions could also be interleaved in the source.
This would require experimentation across a selection of compilers, compiler versions, and optimisation settings.
My gut feeling is that explicit unrolling would be of little benefit, but that hasn't been proven either way.

=== Multi-part Lookup Tables

Next on the list is providing _Multi-Part Table_ kernels.
These process the data at the natural CPU register-width using multiple 8-bit tables.
For instance on a 32-bit machines this approach would require 4 x 8-bit tables.
The performance of these multi-part tables has been https://github.com/komrad36/CRC[measured].
The 4-part tables are around 2.5x faster than a single 8-bit table, and the 8-part tables are around 4.4x times faster.

These come at the cost of a linear increase in the table footprint,
which isn't a big deal on modern PCs but is a *show-stopper* on embedded platforms.
They also have practical speed limits dictated by the speed on the L1 data-cache.
For instance, a 16-part 8-bit table for CRC-32 works out to be 16kB,
which equates to 50% of the L1 data-cache on typical PC CPUs.

=== Intel SSE 4.2 CRC32-C

The Intel SSE 4.2 instruction set provides hardware support for the CRC-32/ISCSI algorithm (CRC32-C).
This uses the Castagnoli polynomial, which is HD6 on datawords up to 655 bytes.
This is far superior to the CRC-32/ISO_HDLC polynomial, which makes the SSE implementation particularly attractive.

GCC, Clang, and MSVC all provide the _mm_crc32_u{8,16,32} family of intrinsics to directly access these SSE CRC instructions.
This greatly simplifies implementation.

Depending on where you look the SSE based implementation is around 15 to 19 times faster than the 8-bit table kernel.

Kareem Omar (GH komrad36) provides a https://github.com/komrad36/CRC[hand-optimised SSE 4.2 implementation] that minimises instruction latency.
This MIT licensed implementation is around 56x faster than the 8-bit table kernel, which is very impressive!
Well done Kareem!

=== STM32 Cortex-M CRC Peripherals

The STM family of ARM Cortex-M microcontrollers provide memory mapped CRC peripherals.
The F1, F2, F4, and L1 variants support CRC-32/MPEG-2.
More recent variants provide support for arbitrary polynomials with CRC-7, CRC-8, CRC-16, and CRC-32 algorithms.
These variants also support DMA, which allows for asynchronous operation with OS (external) support.

STM claims a 60x speed improvement over the bit-at-a-time implementation, which is around 4 to 5 times slower than an 8-bit table lookup.
Another way of looking at this is the STM peripheral is around 12-15x faster than the 8-bit table kernel and around 30x faster than the 4-bit table kernel.

=== ESP32 ROM CRC Tables

The ESP32 family of microcontrollers provide https://github.com/espressif/esp-idf/blob/master/components/esp_rom/include/esp32/rom/crc.h[CRC polynomial lookup tables in ROM] for:

* `x^8 + x^2 + x^1 + 1`
* `x^16 + x^12 + x^5 + 1`
* `x^32 + x^26 + x^23 + x^22 + x^16 + x^12 + x^11 + x^10 + x^8 + x^7 + x^5 + x^4 + x^2 + x^1 + 1`

The driver for these tables allows both the initial value and refin/refout parameters to be specified,
which allows for the simple implementation of many standard CRC algorithms, eg. CRC-16/CCITT and CRC-32/ISO_HDLC.

I suspect the ESP32 facilty is mostly a space saving rather than a speed improvement over the 8-bit RAM based table lookup.
However, since these devices have limited RAM this could represent a practical 2x speed improvement when going from a 4-bit RAM based table to the ROMS based alternative.

=== More Hardware Acceleration Options

Specialised instructions like `CLMUL` on x86 and ARM, or `XMULX` on SPARC are designed speed up multiplication of polynomials over the finite field GF(2).  http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf[Intel published a paper] describing how `PCLMULQDQ` can be used to speed up CRC calculations for generic polynomials.

Other brands of microcontrollers have similar hardware peripherals to those discussed here for STM32/Cortex-M and ESP32.
I won't detail them all here, suffice it to say that each has to be implemented on a case by case basis via a "driver".
